{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Cogmaster, Neuro 102: hands on (f)MRI\n",
    "__Content creator:__ Florent Meyniel, NeuroSpin, CEA Paris-Saclay\n",
    "\n",
    "I would like to acknowledge my colleagues [Le Ster _et al._ (2019)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0225286) for sharing their data on the OpenNeuro platform ([here](https://openneuro.org/datasets/ds002606)). This notebook analyzes a sample participant from this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# First steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use a Python notebook?\n",
    "If you are running this notebook in Google colab, first make sure that you are connected to a computer. If \"Connected\" or \"RAM and disk\" appears in the top, right-hand side corner, you are fine! otherwise, click on \"Connect\".\n",
    "\n",
    "You can skip the remainder of this section if you already know about Python notebook (jupyter-notebook, google colab).\n",
    "\n",
    "The goal of today's session is *not* to learn how to program with Python. However, we will use Python to run some examples and do computations, so here is a quick introduction to Python notebook.\n",
    "\n",
    "A notebook mixes text, lines of code that can be executed, and results that are displayed.\n",
    "\n",
    "There are different ways to execute the code in a cell. All are equivalent:\n",
    "- put the cursor in the cell of code and press \"SHIFT\"+\"ENTER\".\n",
    "- or click on the \"play\" icon (the triangle) on the left-hand side of the cell.\n",
    "\n",
    "For example, the next line asks Python to compute \"1+1\". Execute the line of code to display the (expected) result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number in squared brackets indicates the order in which cells was executed. If you execute again the cell above, you will see that the number within the squared brackets increases. \n",
    "\n",
    "Now, you are going to type your own piece of code in the empty line below. Let's try 2+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to insert your own new cell and enter your code there, click on the \"+\" button in the menu bar at the top of this page. Insert a new cell below and a new code, e.g. 2+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up the environment\n",
    "\n",
    "The following cell installs the libraries that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install libraries\n",
    "! pip install -U nilearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell downloads the data that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone the git repository to get the data\n",
    "! git clone https://github.com/florentmeyniel/cogmaster_neuro102.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the download is complete. When you see the folder:\n",
    ">cogmaster_neuro102\n",
    "\n",
    "in the directory (in the left panel), you can continue and execute the next cell to move in the folder containing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd cogmaster_neuro102/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do various imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm \n",
    "\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "from nilearn.plotting import view_img\n",
    "from nilearn.glm.first_level import run_glm\n",
    "from nilearn.glm import compute_contrast\n",
    "from nilearn.glm import fdr_threshold\n",
    "from nilearn import plotting\n",
    "from nilearn.maskers import NiftiMasker\n",
    "from nilearn import surface\n",
    "from nilearn import datasets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Different subjects, different brains\n",
    "The study by Le Ster and colleagues, as most neuro-imaging studies, is performed at the level of a group of subjects in order to estimate the statistical significance of the results. \n",
    "\n",
    "In a new tab, open the [link to the data stored on OpenNeuro](https://openneuro.org/datasets/ds002606).\n",
    "\n",
    "**Q1-1: How many participants were included in the study?**\n",
    "\n",
    "We are going to explore the data set. Browse the data: open sub_03 / ses-01 / anat and open the anatomical image of this subject by clicking on the \"view\" icon (open it in a new tab). Or alternatively, click [here](https://openneuro.org/datasets/ds002606/versions/1.0.1/file-display/sub-03:ses-01:anat:sub-03_ses-01_T1w.nii.gz) (again, open this link in a new tab).\n",
    "\n",
    "**Q1-2: Knowing that this is an anatomical image, what kind of weighting was used? T1 or T2?**\n",
    "\n",
    "**Q1-3: What do you see in the anatomical image?**\n",
    "\n",
    "You can navigate in this image by moving the cursor.\n",
    "\n",
    "**Q1-4: Can you see the nose, eyes, ears of the participants? Why?**\n",
    "\n",
    "In another tab, display the anatomy of the [subject-04](https://openneuro.org/datasets/ds002606/versions/1.0.1/file-display/sub-04:ses-01:anat:sub-04_ses-01_T1w.nii.gz).\n",
    "\n",
    "**Q1-5: Compare the two anatomies. Do they look similar?**\n",
    "\n",
    "In order to compare quantitatively the anatomy of those two participants, we are going to measure the distance between two anatomical landmarks: the anterior and posterior commissures.\n",
    "\n",
    "**Presentation of the landmarks**\n",
    "\n",
    "Beyond this exercise, it is interesting to know about those landmarks, because the most widely used frames of reference for brain anatomy uses:\n",
    "- the anterior commissure as its origin.\n",
    "- the axis passing through the anterior and posterior commissures as the antero-posterior axis (labeled y). The plane that contains this axis and that is oriented so as to separate the left and right hemispheres is the sagital plane.\n",
    "- the axis orthogonal to the sagital plane corresponds to the left-right axis (labeled x), it is embedded in the axial plane\n",
    "- the axis orthogonoal to those two axes is the bottom-to-top axis (labeled z).\n",
    "This frame of reference is used by the [Talairach](https://en.wikipedia.org/wiki/Talairach_coordinates) and the Montreal Neurological Institute [(MNI)](http://www.bic.mni.mcgill.ca/~louis/stx_history.html) coordinate systems.\n",
    "\n",
    "Try to locate the anterior and posterior commissures using the online viewer and [subject-03](https://openneuro.org/datasets/ds002606/versions/1.0.1/file-display/sub-03:ses-01:anat:sub-03_ses-01_T1w.nii.gz). In order to locate those two landmarks, use the description and pictures provided by this [guide](https://neuroimage.usc.edu/brainstorm/CoordinateSystems) (see the section \"Talairach coordinates\").\n",
    "\n",
    "**Measure the distance**\n",
    "\n",
    "Unfortunately, the image viewer on openneuro.org no longer displays the coordinates of the crosshairs, so we cannot measure the distance between those two landmarks!\n",
    "To get the coordinates, we will use a dedicated software: *MRICron*. here are two options:\n",
    "* Try MRICron yourself. \n",
    "    + Download [MRICron software](https://www.nitrc.org/projects/mricron/), selecting the version (Linux/Mac/Windows) corresponding to your system (I tested the one marked 2-september-2019 for linux, it worked fine).\n",
    "    + Download the anatomical images of our two sample participants from the openneuro website (or use [subject-03](https://openneuro.org/crn/datasets/ds002606/files/sub-03:ses-01:anat:sub-03_ses-01_T1w.nii.gz) and [subject-04](https://openneuro.org/crn/datasets/ds002606/files/sub-04:ses-01:anat:sub-04_ses-01_T1w.nii.gz) for direct download).\n",
    "    + Open MRIcron: unzip the folder you just downloaded, and click on MRICron to launch the software.\n",
    "    + Load the anatimical image (the T1w.nii.gz you downloaded from openneuro) of one subject, using *File>Open>Your file*. Look for the anterior and posterior commissures and mark down their (voxel) corrdinates (X, Y, Z that appear on top). Load the anatomical image of the other subject and repeat the same steps.\n",
    "* Wait for the course instructor to use MRICron. In this case, in the meantime, locate the anterior and posterior commissures of [subject-04](https://openneuro.org/datasets/ds002606/versions/1.0.1/file-display/sub-04:ses-01:anat:sub-04_ses-01_T1w.nii.gz) using the online viewer.\n",
    "\n",
    "To compute the distance between two landmarks A and B, for each subject, we use the following formula:\n",
    "$\\sqrt{(A_x - B_x)^2 + (A_y - B_y)^2 + (A_z - B_z)^2 }$ which is implemented in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(A, B):\n",
    "    return np.sqrt(np.sum([(a-b)**2 for a, b in zip(A, B)]))\n",
    "\n",
    "# Subject #3 (replace AC and PC with the values that you have marked down)\n",
    "AC = [0, 0, 0]\n",
    "PC = [0, 0, 0]\n",
    "print('Subject #3, AC to PC=', f\"{distance(AC, PC):.2f}\", ' voxels')\n",
    "\n",
    "# Subject #4 (replace AC and PC with the values that you have marked down)\n",
    "AC = [0, 0, 0]\n",
    "PC = [0, 0, 0]\n",
    "print('Subject #4, AC to PC=', f\"{distance(AC, PC):.2f}\", ' voxels')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1-6: Which subject has the largest brain among those two participants?**\n",
    "\n",
    "**Q1-7: What problems do you foresee for group-level analysis?**\n",
    "\n",
    "In order to ensure a voxel-to-voxel correspondence across participants, we normalize the anatomy. The simplest way of normalizing anatomy is to move (translation, rotation) and stretch (along the three possible axes) the brain of each subject so as to best match the anatomy of another subject that serves as reference. \n",
    "\n",
    "In practice, the reference anatomy is not from a subject in our group, but is a reference anatomy used by everybody in the research community. Having a common reference anatomy across studies ensures that the brain coordinates are the same across studies. The Montreal Neurological Institute has created such a reference anatomy which is widely used nowadays. \n",
    "\n",
    "The normalization is estimated based on the anatomical image of the subject, and then applied to the functional images, such that the coordinate system of the functional images becomes normalized too.\n",
    "\n",
    "If you have completed this section and need to wait before doing the next one, or if you want to learn more, you can visit:\n",
    "- this online version of the [text-book](https://www.fil.ion.ucl.ac.uk/spm/doc/books/hbf2/), especially section 2-3\n",
    "- those [slides](https://www.fil.ion.ucl.ac.uk/spm/course/slides19-may/), especially 01_fMRI_preprocessing.pptx\n",
    "\n",
    "that are provided by the SPM team at University College London."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of data\n",
    "\n",
    "Normalization is one of the many steps of a standard pre-preprocessing pipeline.\n",
    "In this TD, we are going to use data that have been pre-processed following those steps:\n",
    "- slice timing correction: timeseries in each slice are corrected so as to have the same timing (i.e. as if they had been sampled at the same time). \n",
    "- realignment: consecutive volumes are realigned to correct for volume-to-volume movement of subjects\n",
    "- distortion of corrections induced by local field inhomogeneity\n",
    "- coregistration of the functional and anatomical images\n",
    "- normalisation of the anatomical image to a standard template. The same deformation is applied to functional images.\n",
    "- spatial smoothing of the data.\n",
    "- the data is spatially resampled, from 1.6 mm to 4 mm isotropic.\n",
    "\n",
    "**Q1-8: Comment each step: why do we do that?**\n",
    "\n",
    "Note that I reduced the spatial resolution in order to speed up computations. This is just for educational purposes; in practice, higher resolution is an interesting feature that one wants to leverage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Regression analysis and the General Linear Model (GLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data set: A simple functional \"localizer\"\n",
    "\n",
    "The data we are going to analyze were collected during a \"localizer\" paradigm, developped by [P. Pinel et al, BMC 2007](https://bmcneurosci.biomedcentral.com/articles/10.1186/1471-2202-8-91).\n",
    "\n",
    "\n",
    "Here is (a portion of) the abstract of this paper:\n",
    ">Although cognitive processes such as reading and calculation are associated with reproducible cerebral networks, inter-individual variability is considerable. Understanding the origins of this variability will require the elaboration of large multimodal databases compiling behavioral, anatomical, genetic and functional neuroimaging data over hundreds of subjects. With this goal in mind, we designed a simple and fast acquisition procedure based on a 5-minute functional magnetic resonance imaging (fMRI) sequence that can be run as easily and as systematically as an anatomical scan, and is therefore used in every subject undergoing fMRI in our laboratory. This protocol captures the cerebral bases of auditory and visual perception, motor actions, reading, language comprehension and mental calculation at an individual level.\n",
    "\n",
    "Here is a figure of the paper that describes the task:\n",
    "![pinel](https://media.springernature.com/full/springer-static/image/art%3A10.1186%2F1471-2202-8-91/MediaObjects/12868_2007_Article_389_Fig1_HTML.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a design matrix\n",
    "We will build a design matrix that corresponds to this task. The following file lists the events and their timing in the task.\n",
    "\n",
    "Execute the code that loads the data and browse its content.  \n",
    "**Q2-1: Given the label of trials, is this design amenable to categorical or parametric regressors?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load event file\n",
    "task_file = 'sub-01_ses-01_locAP-sms-1-6iso-events.tsv'\n",
    "events = pd.read_table(task_file)\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will construct a design matrix for this task.  \n",
    "Note that the ten first columns correspond to task-related regressors.  \n",
    "**Q2-2: Explain how those regressors are constructed**  \n",
    "\n",
    "The design matrix also has columns mvt0 to mvt5 corresponding to the volume-to-volume movements of subjects (3 parameters for translation along x, y, z axis, and 3 parameters for rotation pitch, roll, yaw), and drift parameters (drift_1 to drift_4).  \n",
    "**Q2-3: Why do we include those extra regressors in the design matrix?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load movement parameters\n",
    "movement = pd.read_csv('rp_asub-01_locAP_multiband_1_6iso.txt', sep='\\s+',\n",
    "                       header=None, names=[f\"mvt{k}\" for k in range(6)])\n",
    "\n",
    "# get frame times (when each frame, a.k.a. volume, is collected)\n",
    "TR = 1.2285\n",
    "frame_times = np.arange(movement.shape[0])*TR\n",
    "\n",
    "# make design matrix\n",
    "design_matrix = make_first_level_design_matrix(frame_times,\n",
    "                                               events,\n",
    "                                               drift_model=None,\n",
    "                                               add_regs=movement,\n",
    "                                               add_reg_names=[name for name in movement.columns])\n",
    "plot_design_matrix(design_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We following cell of code will select one column: the \"audio_left_hand\" column. The blue curve is the predicted fMRI timecourse elicited by this stimulus, and the dots denote the exact timing of those events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(frame_times, design_matrix['audio_left_hand'], label='predicted BOLD')\n",
    "plt.plot(events.loc[events['trial_type']=='audio_left_hand', 'onset'].values, \n",
    "         [0]*len(events.loc[events['trial_type']=='audio_left_hand', 'onset'].values),\n",
    "         'o', label='left click, audio instruction')\n",
    "plt.legend()\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2-4: We often say that the fMRI response is indirect, delayed and slow. Why?**  \n",
    "**Q2-5: The last peak is nearly twice as large as the other peaks. Why?**\n",
    "Hint: you may want to zoom onto what happened around 250s in the task with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events[(events[\"onset\"]>250) & (events[\"onset\"]<265)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of one voxel\n",
    "We are now going to analyze the activity of a single example voxel. First, load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "masker = NiftiMasker(mask_strategy='whole-brain-template',\n",
    "                    detrend=True,\n",
    "                    high_pass=1/128,\n",
    "                    t_r=TR)\n",
    "fMRI_data = masker.fit_transform('swtrasub-01_locAP_multiband_4mm.nii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extract an example voxel and plot its activity.  \n",
    "**Q2-6: Describe the timeseries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's focus on one voxel\n",
    "voxel_index = 4999\n",
    "example_voxel = fMRI_data[:, voxel_index]\n",
    "\n",
    "# and plot it's activity\n",
    "plt.plot(frame_times, fMRI_data[:, voxel_index])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('BOLD signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will compute and plot the predicted fMRI signal triggered by three types of events:\n",
    "- a click with the left hand\n",
    "- a click with the right hand\n",
    "- flashing a checkerboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction signal\n",
    "predicted = {'left_click': design_matrix['audio_left_hand'].values + design_matrix['video_left_hand'].values,\n",
    "             'right_click':design_matrix['audio_right_hand'].values + design_matrix['video_right_hand'].values,\n",
    "              'checkerboard': design_matrix['horizontal_checkerboard'].values + \\\n",
    "             design_matrix['vertical_checkerboard'].values}\n",
    "\n",
    "for k, stimulus in enumerate(predicted):\n",
    "    plt.subplot(3,1,k+1)\n",
    "    plt.plot(frame_times, predicted[stimulus])\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('BOLD signal')\n",
    "    plt.title(stimulus)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2-7: Compare visually the observed timeseries and those three predicted timeseries. Which one is most similar to the observed timeseries?**  \n",
    "\n",
    "The following cell plots the observed and predicted signal one against the other.  \n",
    "**Q2-8: Interpret the X-Y plot below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted vs. observed\n",
    "for k, stimulus in enumerate(predicted):\n",
    "    plt.subplot(1,3,k+1)\n",
    "    plt.plot(predicted[stimulus], example_voxel, '.')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Observed')\n",
    "    plt.title(stimulus)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2-9 What is a regression weight (a.k.a. \"beta\")?**  \n",
    "The cell below computes and plots the (ordinary least square) estimate of the regression weights.  \n",
    "Note that I use the formula below. In practice, this is packaged in your analysis software (as we will see later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize regressors\n",
    "X = design_matrix.values\n",
    "Xz = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "Xz[:,-1] = 1\n",
    "\n",
    "# Compute the regression weights\n",
    "beta = np.linalg.inv(Xz.T @ Xz) @ Xz.T @ example_voxel\n",
    "plt.bar(np.arange(len(beta)), beta)\n",
    "plt.xticks(ticks=np.arange(len(beta)),\n",
    "          labels=[name for name in design_matrix.columns],\n",
    "          rotation=60, ha='right')\n",
    "plt.ylabel('beta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regressors have been normalized so that the magnitude of the betas can be compared among each other.  \n",
    "**Q2-10: Interpret the betas. What appears to be a likely cause of the activity in this voxel?**  \n",
    "\n",
    "In the course, we saw that we can test effects in the data with contrasts. Below I specify two example contrasts:\n",
    "- \"left click elicits more signal than right click\" (displayed)\n",
    "- \"audio stimuli elicits more signal than visual stimuli\" (not displayed below).  \n",
    "\n",
    "**Q2-11: How is a contrast constructed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify contrast\n",
    "contrasts = {\n",
    "    'left_hand': np.array([1 if 'left_hand' in name else 0 for name in design_matrix.columns]),\n",
    "    'right_hand': np.array([1 if 'right_hand' in name else 0 for name in design_matrix.columns]),\n",
    "    'audio': np.array([1 if 'audio' in name else 0 for name in design_matrix.columns]),\n",
    "    'video': np.array([1 if 'video' in name else 0 for name in design_matrix.columns]),}\n",
    "contrasts['left - right click'] = contrasts['left_hand'] - contrasts['right_hand']\n",
    "contrasts['audio - video'] = contrasts['audio'] - contrasts['video']\n",
    "\n",
    "# Show contrast\n",
    "print(contrasts['left - right click'])\n",
    "plt.bar(np.arange(len(contrasts['left - right click'])),\n",
    "        contrasts['left - right click'])\n",
    "plt.xticks(ticks=np.arange(len(contrasts['left - right click'])),\n",
    "          labels=[name for name in design_matrix.columns],\n",
    "          rotation=60, ha='right')\n",
    "plt.title('Contrast: '+ 'left - right click')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the name of the columns in the design matrix, **Q2-12: What does the following contrast correspond to?**\n",
    "[1 0 0 -1 0 0 1 0 0 -1 0 0 0 0 0 0 0 0 0 0 0]  \n",
    "\n",
    "Below I compute the t-value corresponding to the two contrasts \"left - right click\" and \"audio - video\".  \n",
    "**Q2-13: Which t-value is the largest? Interpret**  \n",
    "\n",
    "Note that I use the formula for educational purpose. This computation is also packaged in standard analysis softwares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the t-value of two contrast\n",
    "def compute_tvalue(y, X, beta, c):\n",
    "    c = c[:, np.newaxis]\n",
    "    res = y - X @ beta\n",
    "    tval = c.T @ beta / (np.std(res) * np.sqrt(c.T @ np.linalg.inv(X.T @ X) @ c))\n",
    "    return tval[0][0]\n",
    "\n",
    "for contrast_id in ['left - right click', 'audio - video']:\n",
    "    print(contrast_id, ', tvalue=',\n",
    "          compute_tvalue(example_voxel, Xz, beta,\n",
    "                         contrasts[contrast_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If time permits, you can rerun the cells above, starting from \"# let's focus on one voxel\" where you change the index of the voxel (the original value is 4999, you can pick any value from 0 to 26942, which is the number of voxels in this data set). Execute the cells in the order in which they appear.\n",
    "\n",
    "In pratice, one does not compute the beta weights and tvalues by hand like I did here, but instead uses existing packages.\n",
    "For instance here is a [regression function](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) available in scikit-learn that returns the beta estimates.\n",
    "And another regression function that returns the beta estimates and their statistics (t-value, p-value, etc.) can be found in [statsmodel](https://www.statsmodels.org/stable/regression.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Whole brain analysis of the \"localizer\" with encoding models  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation and contrast: Testing effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to analyze all voxels, not just an example.  \n",
    "**Q3-1: How does one call this type of regression which is repeated for every voxel?**\n",
    "\n",
    "We are going to use the same contrasts introduced above (plotted below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the contrasts\n",
    "for k, contrast_id in enumerate(['left - right click', 'audio - video']):\n",
    "    plt.subplot(2,1,k+1)\n",
    "    plt.bar(np.arange(len(contrasts[contrast_id])),\n",
    "            contrasts[contrast_id])\n",
    "    plt.plot([0, len(contrasts[contrast_id])-1], [0, 0], 'k')\n",
    "    plt.xlim(-0.5, len(contrasts[contrast_id])-0.5)\n",
    "    plt.title(contrast_id)\n",
    "plt.xticks(ticks=np.arange(len(contrasts['left - right click'])),\n",
    "          labels=[name for name in design_matrix.columns],\n",
    "          rotation=60, ha='right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line of code uses a function from nilearn, a Python package, to estimate the general linear model (GLM) on all voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate GLM on all voxels\n",
    "labels, estimates = run_glm(fMRI_data, design_matrix.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function run_glm does actually a bit more than the simple ordinary least square (OLS) solution used above with the formula. Check the help of the function (by running the next cell). It uses the option with ar1, which stands fro auto-regressive model.  \n",
    "**Q3-2 Why do we use an auto-regressive model for the estimation?**  \n",
    "Note that the OLS can also be used (but it is not the default option of run_glm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_glm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrated the temporal autocorrelation of the signal, we can plot the autocorrelation function for an example voxel.  \n",
    "**Q3-3 What is the autocorrelation in this voxel (correlation between successive values)?**\n",
    "NB: What is problematic for regression model is actually the autocorrelation of the residual timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and plot the autocorrelation function of an example voxel\n",
    "TR_range = range(1, 20)\n",
    "auto_correlation = [np.corrcoef(example_voxel[:-lag], example_voxel[lag:])[0, 1]\n",
    "                    for lag in TR_range]\n",
    "plt.plot(TR_range, auto_correlation, '-o')\n",
    "plt.ylabel('correlation')\n",
    "plt.xlabel('lag (# scans)')\n",
    "plt.grid(visible=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell computes the t-value of the contrast. This is similar to the computation done by hand above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate contrast\n",
    "contrast_id = 'left - right click'\n",
    "contrast = compute_contrast(labels, estimates,\n",
    "                            contrasts[contrast_id],\n",
    "                            contrast_type='t')\n",
    "t_val = masker.inverse_transform(contrast.stat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to display the thresholded T-map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot glass brain\n",
    "plotting.plot_glass_brain(t_val, threshold=3, plot_abs=False,\n",
    "                          colorbar=True, title=contrast_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3-4 What is a glass brain representation?**  \n",
    "The following cell renders the results on slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot slices interactively\n",
    "view_img(t_val,\n",
    "         threshold=3, title=contrast_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3-5: What is the orientation of the slices presented above?**  \n",
    "\n",
    "The following cell renders the results on the surface of the right hemisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get canonical surface anatomy\n",
    "fsaverage = datasets.fetch_surf_fsaverage()\n",
    "\n",
    "# project the results onto the mesh of cortical surface\n",
    "texture = surface.vol_to_surf(t_val, fsaverage.pial_right)\n",
    "\n",
    "# plot\n",
    "plotting.plot_surf_stat_map(\n",
    "        fsaverage.infl_right, texture, hemi='right',\n",
    "        title=contrast_id, colorbar=True,\n",
    "        threshold=3.0, bg_map=fsaverage.sulc_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3-6: What do the blue-ish and red-ish blobs correspond to?**  \n",
    "**Q3-7: Interpret the results.**  \n",
    "\n",
    "The next cell now estimates and reports the \"audio-visual\" contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate contrast\n",
    "contrast_id = 'audio - video'\n",
    "contrast_AV = compute_contrast(labels, estimates,\n",
    "                            contrasts[contrast_id],\n",
    "                            contrast_type='t')\n",
    "t_val_AV = masker.inverse_transform(contrast_AV.stat())\n",
    "plotting.plot_glass_brain(t_val_AV, threshold=3, plot_abs=False,\n",
    "                          colorbar=True, title=contrast_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3-8: Interpret the results.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of different noise models\n",
    "The above estimation and inference assumed autocorrelated noise with an AR one model.  \n",
    "Let's compare the results with the OLS estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate GLM on all voxels\n",
    "labels_OLS, estimates_OLS = run_glm(fMRI_data, design_matrix.values, noise_model='ols')\n",
    "\n",
    "# Estimate contrast\n",
    "contrast_OLS = compute_contrast(labels_OLS, estimates_OLS,\n",
    "                            contrasts[contrast_id],\n",
    "                            contrast_type='t')\n",
    "t_val_OLS = masker.inverse_transform(contrast_OLS.stat())\n",
    "\n",
    "# Plot results\n",
    "plotting.plot_glass_brain(t_val_OLS, threshold=3, plot_abs=False,\n",
    "                          colorbar=True, title=contrast_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(contrast.stat(), contrast_OLS.stat(), '.', ms=0.5)\n",
    "plt.plot([-12, 15], [-12, 15], 'k')\n",
    "plt.xlabel('AR1 estimate')\n",
    "plt.ylabel('OLS estimate')\n",
    "plt.title(f\"T-values\")\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.9 Significance with OLS estimates is biased. In which direction? Why?**   \n",
    "\n",
    "Let's know compare the beta estimates themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(contrast.effect_size(), contrast_OLS.effect_size(), '.', ms=0.5)\n",
    "plt.plot([-100, 200], [-100, 200], 'k')\n",
    "plt.xlabel('AR1 estimate')\n",
    "plt.ylabel('OLS estimate')\n",
    "plt.title('Beta-values')\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.10 Are OLS estimates of beta weights biased? Should we care about the noise model for group-level inference?**   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical significance and correction for multiple comparisons\n",
    "The figure above is displayed with a threshold of +/- 3.  \n",
    "**Q3-11: Change the code to set a more liberal and a more conservative threshold.**  \n",
    "**Q3-12: Do you think that some voxels are actually false positives? Why?**  \n",
    "\n",
    "We are going to use parametric statistics (i.e. the assumption that the t-values follow a known distribution) to assess significance levels as p-values.  \n",
    "The following cell shows the map thresholded at p<0.001 (two-sided test, controlling for both positive and negative effects).  \n",
    "Technical detail: the t-values are transformed into z-values because it is more convenient to do so with the nilearn package.  \n",
    "The z-value corresponding to p<0.001 (two-sided test) uncorrected is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_vals = contrast.z_score()\n",
    "z_map = masker.inverse_transform(z_vals)\n",
    "alpha_threshold = 0.001\n",
    "z_thd = norm.isf(alpha_threshold/2)\n",
    "print(f\"FPR threshold (p={alpha_threshold}, two-sided test): z={z_thd:0.2f}\")\n",
    "plotting.plot_glass_brain(z_map, threshold=z_thd, plot_abs=False,\n",
    "                          colorbar=True, title=contrast_id+' uncorr.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell computes a new threshold that controls for a given false discovery rate (FDR). This estimation is done with the Benjamini-Hochberg procedure. For details on this procedure, and more generally, an introduction to multiple comparisons correction, see this [review by T. Nichols and S. Hayasaka, 2003](https://journals.sagepub.com/doi/pdf/10.1191/0962280203sm341ra).   \n",
    "The threshold (z-value) that corrects for the false discovery rate at p<0.001 (two-sided test) is reported and used to threshold the map.  \n",
    "**Q3-12: Interpret the difference between corrected and uncorrected maps**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr_z_thd = fdr_threshold(np.abs(z_vals), alpha_threshold/2)\n",
    "print(f\"FDR threshold (p={alpha_threshold}, two-sided test): z={fdr_z_thd:0.2f}\")\n",
    "plotting.plot_glass_brain(z_map, threshold=fdr_z_thd, plot_abs=False,\n",
    "                          colorbar=True, title=contrast_id+' FDR corr.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FDR is a weak correction. See the mentioned-aboved review for more conservative corrections. \n",
    "The family-wise error rate (FWER) correction is more conservative. When samples are assumed to be independent, the FWER correction is known as the Bonferroni correction. This correction is overly conservative in fMRI because the signals are spatially smooth (thus, not independent).\n",
    "Here is the result of the Bonferroni correction of our contrast of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_threshold = 0.05\n",
    "fwe_z_thd = norm.isf((alpha_threshold/2)/z_vals.shape[0])\n",
    "print(f\"FWE (Bonferroni) threshold (p={alpha_threshold}, two-sided test): z={fwe_z_thd:0.2f}\")\n",
    "plotting.plot_glass_brain(z_map, threshold=fwe_z_thd, plot_abs=False,\n",
    "                          colorbar=True, title=contrast_id+' FWER Fcorr.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In nilearn, you can use nilearn.glm.threshold_stats_img (with the option 'fpr' for uncorrected, 'fdr' and 'bonferroni') to perform those corrections, and exclude clusters smaller than a given threshold. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm import threshold_stats_img\n",
    "fwer_map_without_small_clusters, fwer_z_thd = threshold_stats_img(\n",
    "    z_map, alpha=alpha_threshold, height_control='bonferroni', cluster_threshold=50)\n",
    "print(f\"FWE (Bonferroni) threshold (p={alpha_threshold}, two-sided test): z={fwe_z_thd:0.2f}\")\n",
    "plotting.plot_glass_brain(fwer_map_without_small_clusters,\n",
    "                          threshold=fwer_z_thd, plot_abs=False,\n",
    "                          colorbar=True, title=contrast_id+' FWER corr.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
